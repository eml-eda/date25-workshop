{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATE 2025 (W07.4)\n",
    "# Pushing TinyML Forward: End-to-end Near-Memory RISC-V Computing\n",
    "---\n",
    "\n",
    "<img src=\"assets/logo.png\" alt=\"MATCH logo\" width=\"1000\"/>\n",
    "\n",
    "---\n",
    "**MATCH** (**M**odel-**A**ware **T**VM-based **C**ompiler for **H**eterogeneous hardware) is a DNN compiler that exploits [Apache TVM](https://tvm.apache.org/)'s BYOC framework, targeting the optimized deployment of DNN applications onto heterogeneous edge System-on-Chip (SoC) platforms.\n",
    "\n",
    "MATCH is designed to make the process of supporting new targets as simple as possible, thanks to a compilation process guided by _high-level model-based hardware abstractions_. \n",
    "\n",
    "The rest of this notebook shows how to extend MATCH to support the deployment of simple DNNs on the _**ARCANE**_ SoC.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Set the required paths and import needed classes and functions from MATCH:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import match\n",
    "from match.target.target import MatchTarget\n",
    "from match.target.memory_inst import MemoryInst\n",
    "from match.transform.requant import MatchRequantRewriter\n",
    "from match.target.exec_module import ExecModule, ModuleLib\n",
    "from match.partition.partitioning_pattern import PartitioningPattern\n",
    "from match.target.exec_module import ComputationalApis\n",
    "from match.node.node import MatchNode\n",
    "from match.utils.utils import get_default_inputs\n",
    "from match.model.model import MatchModel\n",
    "from tvm.relay.dataflow_pattern import wildcard, is_op, is_constant\n",
    "from tvm import relay\n",
    "import onnx\n",
    "\n",
    "from utils import plot_results\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "CURR_PATH = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a new target in MATCH\n",
    "\n",
    "The first step to support a new SoC in MATCH is to define:\n",
    "- The entire SoC as a subclass of the generic `MatchTarget` class.\n",
    "- Each accelerator (only one in this case, i.e., Arcane) as a subclass of `ExecModule`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining an Accelerator (`ExecModule`)\n",
    "\n",
    "The code below contains the definition of the Arcane class:\n",
    "- The constructor defines the accelerator name and points to the paths of any required support library that should be compiled together with the generated inference code.\n",
    "- The `partitioning_patterns` method defines DNN graph patterns that this accelerator supports, each with optional constraints on geometry, data format, etc. In this example, for simplicity, we support `dense` operations with `int32` format, but in general we can match any number of patterns of arbitrary complexity.\n",
    "- The `comp_apis_def` method defines the APIs of the accelerator (in this case, only the computation APIs, but in general we also support APIs for accelerator setup, data transfer, etc). Essentially, for each pattern supported by the accelerator, it points to backend functions to execute the corresponding operation.\n",
    "- The `update_constants` method is overridden to modify the default data layout for constant tensors (i.e., weights) of TVM. This is needed because Arcane expects the weights of dense layers to be stored in the CN layout(input channels, output channels) instead of the default (NC).\n",
    "- The `include_list` method defines extra include paths for compilation.\n",
    "- If necessary for DSE, you can also override the `zigzag_cost_model` to return an instance of `match.cost_model.zigzag.ZigZagMatchCostModel` as detailed in the documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arcane(ExecModule):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name = \"arcane\",\n",
    "            libs_required = {\n",
    "                \"arcane_helper\": ModuleLib(name=\"arcane_helper\", base_path=CURR_PATH+\"/arcane_helper\"),\n",
    "            },\n",
    "        )\n",
    "    \n",
    "    def partitioning_patterns(self):\n",
    "        def dense_pt():\n",
    "            \"\"\"Create pattern for matmul.\"\"\"\n",
    "            dense = is_op(\"nn.dense\")(\n",
    "                wildcard(), is_constant()\n",
    "            )\n",
    "            return dense\n",
    "        def only_int32_dense(node):\n",
    "            dense = node\n",
    "            if dense.checked_type.dtype != \"int32\":\n",
    "                return False\n",
    "            return True\n",
    "        return [\n",
    "            PartitioningPattern(\n",
    "                name=\"DENSE_PT\",\n",
    "                pattern=dense_pt,\n",
    "                additional_checks=only_int32_dense\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "    def comp_apis_def(self, computational_apis: ComputationalApis=None, pattern_name = \"dense\"):\n",
    "        computational_apis.compute_tile = \"arcane_compute_wrapper\"\n",
    "        return computational_apis\n",
    "    \n",
    "    def update_constants(self, match_node: MatchNode=None, pattern_name: str=\"dense\"):\n",
    "        for w_tensor in match_node.const_tensors.values():\n",
    "            if \"dense\" in w_tensor.name:\n",
    "                if w_tensor.layout!=\"CN\":\n",
    "                    w_tensor.data = w_tensor.data.transpose(1,0)\n",
    "                    w_tensor.dims = [w_tensor.dims[1], w_tensor.dims[0]]\n",
    "                w_tensor.layout = \"CN\"\n",
    "                \n",
    "    def include_list(self):\n",
    "        return [\"arcane_helper/arcane_helper\"]\n",
    "\n",
    "    # def zigzag_cost_model(self):\n",
    "    #     return ExampleCostModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Full SoC (`MatchTarget`)\n",
    "\n",
    "Next, we define the entire SoC, which in this case is based on the [X-Heep](https://github.com/esl-epfl/x-heep) platform, as a `MatchTarget`. In this case, the constructor defines:\n",
    "- The constructor defines SoC name and the list of its accelerators (`exec_modules`), as well as various paths and function names for compilation.\n",
    "- The `network_transformations` methods defines a set of custom DNN graph transformation passes to be executed when targeting this SoC. In our case, we use the built-in `MatchRequantRewriter` to replace demanding division operations during requantization with right shifts. Any number of transformations can be defined.\n",
    "- The `host_memories` method defines the memory hierarchy of the target (visible to TVM), which in this case contains a single-level L2 memory composed of 1024 32-bit words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCANE_L2_MEMORY_SIZE = 8*32*1024\n",
    "\n",
    "class XHeepSoC(MatchTarget):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            exec_modules = [\n",
    "                Arcane()\n",
    "            ]\n",
    "            , name = \"xheepsoc\",\n",
    "        )\n",
    "        self.makefile_path = CURR_PATH+\"/arcane_lib/Makefile\"\n",
    "        self.tvm_runtime_include_path = CURR_PATH+\"/arcane_lib/tvm_runtime.h\"\n",
    "        self.tvm_runtime_src_path = CURR_PATH+\"/arcane_lib/tvm_runtime.c\"\n",
    "        self.init_funcs = [\"arcane_helper_init_l1_mem\"]\n",
    "        self.include_list = [\"arcane_helper/arcane_helper\"]\n",
    "\n",
    "    def network_transformations(self, opts):\n",
    "        return [\n",
    "            (\"requant\", MatchRequantRewriter()),\n",
    "        ]\n",
    "\n",
    "    def host_memories(self):\n",
    "        return [\n",
    "            MemoryInst(name=\"ARCANE_L2_MEM\", k_bytes=ARCANE_L2_MEMORY_SIZE),\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `arcane_helper` Plugin Library\n",
    "MATCH's template-based compilation requires each accelerator to provide a backend library of elementary inference operations (usually in C/C++). This library clearly depends on the target, and is typically provided by the HW manufacturers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In our simple example, we just need to define two functions:\n",
    "- To initialize the accelerator\n",
    "- To perform computation (of a dense layer).\n",
    "Additional functions can be defined to support more DNN operations.\n",
    "\n",
    "```c\n",
    "void arcane_helper_init_l1_mem(){\n",
    "    l1_hal_init(0, l1_loader, sizeof(l1_loader), 1);\n",
    "}\n",
    "\n",
    "void arcane_compute_wrapper(MatchCtx* ctx){\n",
    "    MatchTensor* tensors = ctx->tensors->tensors;\n",
    "    int num_tensors = ctx->tensors->num_tensors;\n",
    "    int out_chs = tensors[num_tensors-1].tiles[1].size;\n",
    "    int inp_chs = tensors[0].tiles[1].size;\n",
    "    // reserve matrix in the ARCANE NMC module for the activations\n",
    "    xmr(m0, tensors[0].base_pt, 1, inp_chs, 1, 1);\n",
    "    // reserve matrix in the ARCANE NMC module for weights stored in CN format\n",
    "    xmr(m1, tensors[1].base_pt, inp_chs, out_chs, 1, 1);\n",
    "    // reserve matrix in the ARCANE NMC module for the outputs\n",
    "    xmr(m2, tensors[num_tensors-1].base_pt, 1, out_chs, 1, 1);\n",
    "    // dense op\n",
    "    carus_mmul_tiling(m2, m0, m1, mNONE, 0, 0);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the DNN\n",
    "\n",
    "Now that MATCH has been extended to support our SoC, we can compile full DNNs with it. In this example, we use an `int32` ONNX model of a MLP network for MNIST digit recognition. This ONNX can be generated with any ONNX-compilant framework. In our case, we used [PLiNIO](https://github.com/eml-eda/plinio/).\n",
    "\n",
    "You can view the network's graph by dragging and dropping the `.onnx` file in [Netron](https://netron.app)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compile it, we define a `MatchModel` as follows, where:\n",
    "- `default_inputs` is an optional parameter used to add a (properly converted) input to the generated code, for testing purposes.\n",
    "- `handle_out_fn` is the name of an optional function used to post-process the network output (in our case, determine the predicted class from the DNN scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_file_path = CURR_PATH+\"/model/mnist.onnx\"\n",
    "input_path = f\"{CURR_PATH}/model/input.txt\"\n",
    "\n",
    "mnist_model = MatchModel(\n",
    "    filename=onnx_file_path,\n",
    "    model_name=\"mnist\",\n",
    "    default_inputs=get_default_inputs(\n",
    "        mod=relay.frontend.from_onnx(onnx.load(onnx_file_path))[0],\n",
    "        input_files=[input_path],\n",
    "    ),\n",
    "    handle_out_fn=\"mnist_handle_output\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the Compilation\n",
    "\n",
    "The compilation can be ran with the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match.match(\n",
    "    model=mnist_model,\n",
    "    target=XHeepSoC(),\n",
    "    output_path=CURR_PATH+\"/output\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Generated Code\n",
    "\n",
    "The generated code is split over multiple files. \n",
    "- `./output/src/main.c` is the entry point, which essentially calls:\n",
    "    * The runtime function that runs the inference (`match_mnist_runtime`).\n",
    "    * The custom output handler function to interpret the results (`match_handle_output`).\n",
    "\n",
    "The runtime function (defined in `./output/src/mnist/runtime.c`), in turn, calls functions define in the `codegen` folder, which contains the most interesting part of the DNN-generated code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls output/codegen/mnist/src/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two files in this folder (`_lib0.c` and `_lib1.c`) contain TVM-generated functions, including constant allocations, lowered code for unsupported operations, and the main inference function. The remaining files (`_lib2.c` etc) contain BYOC-generated code. In our case, this corresponds to the functions implementing the matched dense layers.\n",
    "\n",
    "#### TVM-Generated Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tail -n 18 output/codegen/mnist/src/mnist_lib1.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MATCH-generated Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat output/codegen/mnist/src/mnist_lib2.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Host-only Compilation\n",
    "\n",
    "MATCH allows you to easily disable code generation for accelerators, thus generating a host-only version of the code, which can be useful to measure acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcane_cpu_only = XHeepSoC()\n",
    "arcane_cpu_only.disable_exec_module(\"arcane\")\n",
    "match.match(\n",
    "    model=mnist_model,\n",
    "    target=arcane_cpu_only,\n",
    "    output_path=CURR_PATH+\"/output_cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the codegen folder only contains TVM files, which also include lowered versions of dense layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls ./output_cpu/codegen/mnist/src/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./output_cpu/codegen/mnist/src/mnist_lib1.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Relay IR\n",
    "\n",
    "We can also compare the TVM Relay IR graphs generated with/without support for the Arcane accelerator.\n",
    "\n",
    "In the first case, we offload dense operations to ARCANE exploiting MATCH:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!cat output/models/mnist/relay/mnist.relay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While in the CPU-only version everything is handled by the host:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!cat output_cpu/models/mnist/relay/mnist.relay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested, the `models/mnist/relay` folder contains the evolution of the graph throughout the compilation. For instance, we can see that in the final one (`mnist.relay`)e `right_shift` operations are used instead of divisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on the Hardware\n",
    "\n",
    "To generate the final binary for the target, you need to go into the `output` (or `output_cpu`) folders, from a Docker terminal and compile the generated C code. This step requires a target-specific C compilation environment, and will be skipped for sake of brevity.\n",
    "\n",
    "Let's flash a pre-compiled the board and see if the model works correclty. For reference, the sample input we compiled together with the network is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.loadtxt(input_path).reshape([28, 28])\n",
    "plt.imshow(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acceleration Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We profiled the accelerated and the host-only code and we obtained the following results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
